---

title: "Data Cleaning"
author: "Mathias"
date: "08/09/2020"
output: html_document
---


##### 1 - Get/Set your working directory

Relative : setwd(.\...) or
absolute 

##### 2 - Check for directory:
```{r setup, include=FALSE}
if (!file.exists("./data")){
  dir.create("./data")
}
```

##### 3 - Get/download Data from the Internet

```{r}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/cameras.csv", method="curl")
list.files("./data")
```
```{r}
dateDownloaded <- date()
dateDownloaded
```
##### 4 - Reading local files

```{r}
cameraData <- read.table("./data/cameras.csv", sep = ",", header=TRUE)
head(cameraData)
```
important parameters:
  quote
  na.string (set the issing value character)
  nrows (number of rows to read)
  skip (number of rows to skip)


```{r}

library(rJava)
library(xlsxjars)

cameraData <- read.xlsx("./data/cameras.xlsx", sheetIndex=1, colIndex=2:5, rowIndex=2:5)

```


##### 5 - XML (to be redo !!!!)

```{r}
# install.package("XML")
# library(XML)

fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
```
##### 6 - JSON

```{r}
library(jsonlite)
jsondata <- fromJSON("https://api.github.com/users/BuZ4rD/repos")
head(names(jsondata))
iris2 <- fromJSON(myjson) # to DataFrame
head(iris2)

```

```{r}
names(jsondata$owner)
```

```{r}
myjson <- toJSON(iris, pretty=TRUE)
```
##### 7 - DATA TABLES
much faster than data frames (subsetting...)

```{r}
library(data.table)
DF = data.frame(x=rnorm(9), y=rep(c("a","b","c"), each=3), z=rnorm(9))
head(DF,3)

```
```{r}
DT = data.table(x=rnorm(9), y=rep(c("a","b","c"), each=3), z=rnorm(9))
head(DT,3)
```



```{r}
tables()
```
```{r}
DT[2,]
```

```{r}
DT[DT$y=="a",]
```
```{r}
DT[,c(2,3)]
```
```{r}
DT[,list(mean(x),sum(z))]
```
```{r}
DT[,table(y)]
```
Add a new variable !!!
```{r}
DT[,w:=z^2] # anonym function
DT[1:3,]
```

```{r}

```

















```{r}
Url = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
data <- read.csv(Url)
data <- data.table(data)
head(data)
str(data[,1:10])
```


##### 8 - MySQL

```{r}
install.packages("RMySQL")
library(RMySQL)
ucscDB <- dbConnect(MySQL(), user="genome", host ="genome-mysql.soe.ucsc.edu")
result <- dbGetQuery(ucscDB, "show databases;"); dbDisconnect(ucscDB)
MySQL()

```


##### 10- Scrapping


```{r}
con = url("http://www.dendropolis.org")
htmlCode = readLines(con)
close(con)
head(htmlCode,30)

```
```{r}
library(XML)
library(httr)
url <- "http://www.dendropolis.org"
html<- htmlParse(rawToChar(GET(url)$content))
html

xpathApply(html, "//title", xmlValue)

```
```{r}
xpathApply(html, "//head", xmlValue)
```


Get from the HTTR package
```{r}
library(httr); html2=GET(url)
content2 = content(html2, as="text")
parsedHtml = htmlParse(content2, asText=TRUE)
xpathSApply(parsedHtml, "//head", xmlValue)

```
Accessing with Password
```{r}
pg2 = GET(url,
  authenticate("user", "password"))

```

Use Handles
```{r}
google = handle("http://google.com")
pg1 = GET(handle=google, path="/")
pg2 = GET(handle = google, path="search")

```

r-blogger --- s=web+scrapping


##### 11 - FROM APIs

Apis (like twitter...)

Create an Application (for instance APis in Twitter, giving Oauth and names, Url...)

```{r}
myapp = oauth_app("twitter",
                    key="yourconsumerkey",
                    secret="yourconsumersecrethere")
sig =  sign_oauth1.0(myapp, token = "tyour token here", token_secret = "your token secret here")
homeTL = GET("https://api.twitter.com/1.1/statuses/home", sig)
```


```{r}
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json[1,1:4]

```
Get information online on the REST APIs documentation
httr allows to GET, POST, PUT, DELETE requests (depending authorization)
- authentication
modern APIs use somethink like Oauth





QUIZZ

1-
```{r}
data1 <- data[data$VAL==24,]
nrow(data1)


```
GOOD


2-

BADS
Tidy data has one observation by row

Each Variable in a tidy set has been transformed to be interpretable
```{r}
unique(data[,"FES"])
```
3-

GOOD 36534720


```{r}
install.packages ("openxlsx")
install.packages("rJava")
library(rJava)
library(xlsxjars)
dat <- read.xlsx("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", rows = 15:21, cols = 7:15)
dat


```
```{r}
sum(dat$Zip*dat$Ext,na.rm=T)
```



4-

```{r}
library(XML)
url <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlParse(url, useInternal = TRUE)
dat <- data.table::data.table(doc)
print(doc)

# Load the packages required to read XML files.
undebug(library)
library("XML")
library("methods")


# Exract the root node form the xml file.
rootnode <- xmlRoot(doc)

# Find number of nodes in the root.
rootsize <- xmlSize(rootnode)

# Print the result.
print(rootsize)
```
```{r}
# Exract the root node form the xml file.
rootnode <- xmlRoot(result)

# Print the result.
print(rootnode[1])
```
```{r}
xmldataframe <- xmlToDataFrame(doc)

# Transpose
xmldataframe <- t(xmldataframe)

result <- grepl(21231, xmldataframe[,], fixed = TRUE)
length(result[result==TRUE])
```
```{r}
library(data.table)
dat <- fread("./data/getdata_data_ss06pid.csv")

head(dat[,"SEX"])
class(dat)
```
```{r}
system.time({ tapply(dat$pwgtp15, dat$SEX, mean) })
```
```{r}
system.time({ dat[, mean(pwgtp15), by=SEX] })
```
```{r}
system.time({ sapply(split(dat$pwgtp15, dat$SEX), mean) })
```
```{r}
system.time({ rowMeans(dat)[dat$SEX==1];rowMeans(dat)[dat$SEX==2]})

```
```{r}
system.time({ mean(dat[dat$SEX==1,]$pwgtp15);mean(dat[dat$SEX==2,]$pwgtp15)})
```


